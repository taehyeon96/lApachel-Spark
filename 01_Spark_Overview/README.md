# 01. Spark Overview

---
## 1.1 Spark Basic

- Spark
  - 하둡 기반의 맵리듀스 작업이 가진 단점을 보완하기 위한 것
  - 메모리를 이용한 데이터 저장 방식을 제공 => 머신러닝 등 반복적인 데이터 처리가 필요할 시 높은 성능
  
  
- Spark에서 데이터를 표현하고 처리하기 위한 프로그래밍 모델
  - "RDD", "데이터셋", "데이터프레임"


- RDD : resilient distributed dataset
  - 단순히 "값"으로 표현되는 데이터만 가리키는 것이 아니고,
  - 데이터를 다루는 방법까지 포함하는 일종의 클래스와 같은 개념
  - "RDD란 스파크가 사용하는 핵심 데이터 모델로서 다수의 서버에 걸쳐 분산 방식으로 저장된 데이터 요소들의 집합을 의미하며,
    병렬처리가 가능하고 장애가 발생할 경우에도 스스로 복구될 수 있는 내성(tolerance)를 가지고 있다."
  - "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault tolerant collection of elements that can be operated on in parallel."

  - 즉, RDD는 스파크에서 정의한 분산 데이터 모델로서 병렬처리가 가능한 요소로 구성되며,
    데이터를 처리하는 과정에서 문제가 발생하더라도 스스로 에러를 복구할 수 있는 능력을 가진 모델이다.
    => 첫 글자가 resilient 인 이유
    
    
- RDD를 통해 분산 병렬처리를 하는 방법
  - RDD에 속한 요소들은 "파티션"이라는 단위로 나눠질 수 있다.
  - 따라서 스파크는 작업을 수행할 때 하나의 RDD를 여러 개의 파티션으로 나눠서 병렬처리를 수행
  - 이때 각 파티션을 다수의 서버에 보내어 처리


- 리니지(Lineage)
  - 리니지란, 스파크에서 RDD 생성 작업을 기록해두는 것
  - 다수의 서버에서 작업하다 보니 일부 파티션 처리에 장애가 발생할 수 있는데
  - 이때 스파크는 손상된 RDD를 복원하기 위해 RDD의 "생성 과정을 기록"해둠
  - 즉, RDD에 포함된 데이터를 일일히 저장해뒀다가 복원하는게 아니고, 
    RDD를 생성했던 기록을 저장해서 문제가 발생했던 "직전 시점의 RDD를 만들 때 작업을 똑같이 실행해 데이터를 복구"하는 방식임 
    
    
- RDD 생성 방법 3가지
  - Collection 사용
    - sc.parallelize(리스트 자료형)
  - 파일로부터 생성
    - sc.textFile("파일경로")
  - 기존 RDD로부터 새로운 RDD 생성
    - rdd_B = rdd_A.map(변환할 내용)


- RDD 연산
  - RDD 생성 후 수행하는 두 가지 필수 작업
  - Transformation (트랜스포메이션)
    - 어떤 RDD에 변형을 가해 "새로운 RDD를 생성하는 연산"
    - 이때, 기존 RDD는 바뀌지 않고, 변형된 값을 가진 새로운 RDD가 생성되는 연산임 => resilient를 위해
    - <중요> 해당 연산은 바로 수행되지 않음!! 정보만 가지고 있음!!
  - Action (액션)
    - 연산의 결과가 RDD가 아닌 "int, long 등의 값을 반환하거나 / 아얘 반환하지 않거나 / 결과물 출력하는 등의 연산"
    - 트랜스포메이션이 정보를 누적해서 갖고 있다가 "액션"에 해당하는 연산이 호출될 때 한꺼번에 실행
    - 이를 통해, 본격적인 작업 실행 전에 데이터를 어떤 방법과 절차에 따라 변형할지 정할 수 있음
    - => 특히 쓸데없는 작업 수를 줄여 데이터 처리 시 비용을 줄이고, 셔플(shuffle) 횟수도 줄일 수 있다.






